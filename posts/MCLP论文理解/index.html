<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="这是一个个人博客，记录技术学习和生活感悟"><title> 推荐算法：MCLP论文解读  | RenaXu 's Wonderland</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="/css/custom-icons.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="/css/toc.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="https://unpkg.com/normalize.css"><link rel="stylesheet" type="text/css" href="https://unpkg.com/purecss/build/pure-min.css"><link rel="stylesheet" type="text/css" href="https://unpkg.com/purecss/build/grids-responsive-min.css"><link rel="stylesheet" href="https://unpkg.com/font-awesome@4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="https://unpkg.com/jquery/dist/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="https://unpkg.com/clipboard/dist/clipboard.min.js"></script><script type="text/javascript" src="https://unpkg.com/toastr/build/toastr.min.js"></script><link rel="stylesheet" href="https://unpkg.com/toastr/build/toastr.min.css"><div class="darkmode-toggle">🌓</div><script>var prefersDarkMode = window.matchMedia('(prefers-color-scheme: dark)');
var toggle = document.querySelector('.darkmode-toggle');
var html = document.querySelector('html');

html.dataset.dark = localStorage.dark || prefersDarkMode.matches;

toggle.addEventListener('click', () => {
localStorage.dark = !(html.dataset.dark == 'true');
html.dataset.dark = localStorage.dark;
});</script><meta name="generator" content="Hexo 8.1.1"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden"> 推荐算法：MCLP论文解读 </h1><a id="logo" href="/.">RenaXu 's Wonderland</a><p class="description">分享技术与生活</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-4-4"><div class="content_container"><div class="post"><h1 class="post-title"> 推荐算法：MCLP论文解读 </h1><div class="post-meta">2026-02-10<span> | </span><span class="category"><a href="/categories/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/">推荐算法</a></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 1.9k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 6</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-text"></span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-text">论文概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-text">背景与动机</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-text">核心方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-text">1. 用户偏好挖掘 (User Preference Mining)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-text">2. 到达时间估计 (Arrival Time Estimating)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-text">3. 序列模式挖掘 (Sequential Pattern Mining)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-text">4. 最终预测</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-text">实验分析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-text">主实验结果</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-text">消融实验</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-text">深度理解问答</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-text">Q1: 为什么使用 LDA 提取偏好比直接让模型学习 Embedding 更好？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-text">Q2: 到达时间估计器中，为什么不直接预测一个具体的时间点？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-text">Q3: 模型在处理超长轨迹时表现如何？</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-text">总结与思考</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-text">核心贡献</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-text">局限性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-text">适用场景</span></a></li></ol></li></ol></div></div><div class="post-content"><blockquote>
<p><strong>论文</strong>：Going Where, by Whom, and at What Time: Next Location Prediction Considering User Preference and Temporal Regularity<br><strong>作者</strong>：Tianao Sun, Ke Fu, Weiming Huang, Kai Zhao, Yongshun Gong, Meng Chen (山东大学、南洋理工大学、佐治亚州立大学)<br><strong>发表</strong>：KDD 2024<br><strong>阅读时长</strong>：约 15 分钟<br><strong>难度</strong>：⭐⭐⭐ (需要轨迹挖掘、Transformer 基础)<br><strong>前置知识</strong>：Transformer 架构、LDA 主题模型、人类移动性建模</p>
</blockquote>
<h2><span id></span></h2><p>本文针对下一地点预测（Next Location Prediction）中用户偏好建模浅层化和时间规律利用不足的问题，提出了 <strong>MCLP</strong> 模型。核心创新在于将用户偏好（通过 LDA 挖掘）和下一时刻到达时间（通过注意力机制估计）显式作为上下文信息注入 Transformer 预测框架，在两个真实数据集上显著提升了预测精度。</p>
<h2><span id="论文概述">论文概述</span></h2><p><strong>问题</strong>：现有的移动性预测模型往往将用户偏好简化为随机初始化的向量，且未能充分利用“到达时间”这一关键决定因素，或者仅将其视为辅助任务。</p>
<p><strong>方案</strong>：提出多上下文感知位置预测模型（MCLP），通过显式建模用户偏好和预计到达时间作为核心上下文。</p>
<p><strong>贡献</strong>：</p>
<ol>
<li>提出一种联合建模用户偏好、时间规律和序列模式的新范式。</li>
<li>利用概率主题模型（LDA）提取用户地点偏好作为先验知识。</li>
<li>设计基于多头注意力机制的到达时间估计器，为位置预测提供动态时间上下文。</li>
<li>在交通摄像头和手机信令两个真实数据集上证明了 MCLP 的优越性。</li>
</ol>
<h2><span id="背景与动机">背景与动机</span></h2><p>人类移动性具有高度的规律性，但也存在随机性。下一地点的选择往往受限于“谁在走”和“什么时候走”：</p>
<ul>
<li><strong>用户偏好（By Whom）</strong>：不同用户有不同的生活习惯（如购物达人 vs. 学习爱好者）。传统的 Embedding 方法往往从零开始学习，难以捕捉轨迹中蕴含的显式偏好。</li>
<li><strong>时间规律（At What Time）</strong>：时间是地点的强约束（如中午去餐厅，晚上回家）。以往研究要么忽略到达时间，要么将其作为次要的辅助任务，未意识到预测时间比预测地点容易得多，且时间可以作为预测地点的有力证据。</li>
</ul>
<p>论文通过**移动熵（Mobility Entropy）**分析证明，引入时间上下文后，用户移动模式的熵值显著降低，这意味着预测的可行性和准确性理论上会更高。</p>
<h2><span id="核心方法">核心方法</span></h2><p>MCLP 的整体架构由三个核心组件组成：用户偏好挖掘、到达时间估计和序列模式挖掘。</p>
<p><img src="https://private-us-east-1.manuscdn.com/sessionFile/Dmivc7PvROkeB5xQjeHeBc/sandbox/hvPd9uevNS53IFBWIfk4PO-images_1770790904325_na1fn_L2hvbWUvdWJ1bnR1L3BhcGVyLW5vdGVzL21jbHAvaW1hZ2VzL2ZpZzNfYXJjaF9vdmVyYWxs.png?Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9wcml2YXRlLXVzLWVhc3QtMS5tYW51c2Nkbi5jb20vc2Vzc2lvbkZpbGUvRG1pdmM3UHZST2tlQjV4UWplSGVCYy9zYW5kYm94L2h2UGQ5dWV2TlM1M0lGQldJZms0UE8taW1hZ2VzXzE3NzA3OTA5MDQzMjVfbmExZm5fTDJodmJXVXZkV0oxYm5SMUwzQmhjR1Z5TFc1dmRHVnpMMjFqYkhBdmFXMWhaMlZ6TDJacFp6TmZZWEpqYUY5dmRtVnlZV3hzLnBuZyIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc5ODc2MTYwMH19fV19&Key-Pair-Id=K2HSFNDJXOU9YS&Signature=jTwBWPNYus9iSL480hXLN1rBlsViRfJINqprQ9wRMMpaXxOyklQ~ayPTTB7Itu-bVRnZFd8ITyjPxT9-cSRX~z-xaY2qdojvng24SCmGFvtS~Y8mJ4XrVmHYZPOB0YK7jcjZKAHGKl9Q0-GHo6I6VIljpr3ZvuwAXLdf0zpBS07~1e0Cq4HRtE5b0KlgMma87N8E3Cohl7poB-SZgkzU0bBExCBEclYIqDZwqRbXSY10vvQsfItZVGPdDkuoh9yT~ERIMlHqqAb5vcWVBlqDnuQy9iDVo2qAv7Rh9xPEfhhMh0EOO6RV5HjlS7AbYkOHSrF38o-SPgrKb5EXxiWlUQ__" alt="MCLP 整体架构"></p>
<h3><span id="1-用户偏好挖掘-user-preference-mining">1. 用户偏好挖掘 (User Preference Mining)</span></h3><p>为了注入先验知识，模型将用户轨迹视为“文档”，地点视为“单词”，利用 <strong>LDA (Latent Dirichlet Allocation)</strong> 模型生成用户-主题分布。</p>
<ul>
<li><strong>输入</strong>：用户-地点共现矩阵。</li>
<li><strong>输出</strong>：用户偏好向量 $C^{u_i}$。</li>
<li><strong>处理</strong>：通过一个带有残差连接和 LayerNorm 的 MLP 进一步提升语义表达，得到偏好嵌入 $e_{pre}^{u_i}$。</li>
</ul>
<h3><span id="2-到达时间估计-arrival-time-estimating">2. 到达时间估计 (Arrival Time Estimating)</span></h3><p>这是本文的亮点之一。模型不再直接预测下一地点，而是先估计“什么时候到达”。</p>
<p><img src="https://private-us-east-1.manuscdn.com/sessionFile/Dmivc7PvROkeB5xQjeHeBc/sandbox/hvPd9uevNS53IFBWIfk4PO-images_1770790904325_na1fn_L2hvbWUvdWJ1bnR1L3BhcGVyLW5vdGVzL21jbHAvaW1hZ2VzL2ZpZzRfbWV0aG9kX2Fycml2YWxfdGltZQ.png?Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9wcml2YXRlLXVzLWVhc3QtMS5tYW51c2Nkbi5jb20vc2Vzc2lvbkZpbGUvRG1pdmM3UHZST2tlQjV4UWplSGVCYy9zYW5kYm94L2h2UGQ5dWV2TlM1M0lGQldJZms0UE8taW1hZ2VzXzE3NzA3OTA5MDQzMjVfbmExZm5fTDJodmJXVXZkV0oxYm5SMUwzQmhjR1Z5TFc1dmRHVnpMMjFqYkhBdmFXMWhaMlZ6TDJacFp6UmZiV1YwYUc5a1gyRnljbWwyWVd4ZmRHbHRaUS5wbmciLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3OTg3NjE2MDB9fX1dfQ__&Key-Pair-Id=K2HSFNDJXOU9YS&Signature=izbxRXm-lvwEyT6v0zFeKLBep30IVbdP-ykCpRA~3CPazB~3vThPeW9hquFdHMLD2BTPYW-5A9hvco24fahxmewb2~3PzcxY1ojLUxPMNg5aaUWlO16hsx93o4j8tvHmXLFFqq1Cpxrp1bXrcAYiMIjn0NY6vmwoDRI7stPyXoN-o8oLNuwQXJJGfgKI9kg77hIcBmtsHg439v8vknZSbSJgrwI9xT-aLGYJ~hCJcL2KBBMXDKTIbXnm9G6r-Xnzx5Xe~AtlVm7dje4o9MU5pxURSvWmJcdaerpM9nmqDqU4l1ZCw0-KNaJ~1ZeeUZq1Gz8rQZ944XyuF0eBUhFa4g__" alt="到达时间估计器结构"></p>
<ul>
<li><strong>输入</strong>：用户时间嵌入 $e_t^{u_i}$（识别个性化时间偏好）、当前时间嵌入 $e_t^n$ 和所有候选时段嵌入。</li>
<li><strong>核心机制</strong>：使用多头注意力机制（MHSA），以当前时间和用户个性化时间为 Query，所有候选时段为 Key&#x2F;Value，计算加权聚合的到达时间嵌入 $e_n^{at}$。</li>
<li><strong>设计逻辑</strong>：这种方法比单一时段预测更具鲁棒性，能捕获用户在特定时间段内的行为概率分布。</li>
</ul>
<h3><span id="3-序列模式挖掘-sequential-pattern-mining">3. 序列模式挖掘 (Sequential Pattern Mining)</span></h3><p>使用 <strong>Transformer</strong> 编码器捕捉用户地点的长短期依赖关系。</p>
<ul>
<li><strong>输入序列</strong>：地点和时间的嵌入序列 $X &#x3D; {e_l + e_t + PE}$。</li>
<li><strong>输出</strong>：序列上下文向量 $h_n^{out}$。</li>
<li><strong>增强</strong>：通过残差式连接加入当前位置和时间信息，生成最终的序列嵌入 $e_n^{seq}$。</li>
</ul>
<h3><span id="4-最终预测">4. 最终预测</span></h3><p>将上述四个嵌入向量拼接，通过全连接残差层进行 Softmax 分类：<br>$$P(\hat{l}<em>{n+1}) &#x3D; \text{softmax}(\text{FC}(e</em>{pre}^{u_i} \oplus e_l^{u_i} \oplus e_n^{at} \oplus e_n^{seq}))$$</p>
<h2><span id="实验分析">实验分析</span></h2><h3><span id="主实验结果">主实验结果</span></h3><p>模型在 Traffic Camera 和 Mobile Phone 两个数据集上均取得了 SOTA 性能。</p>
<table>
<thead>
<tr>
<th align="left">方法</th>
<th align="left">Traffic Camera (MRR)</th>
<th align="left">Mobile Phone (MRR)</th>
</tr>
</thead>
<tbody><tr>
<td align="left">DeepMove</td>
<td align="left">46.08</td>
<td align="left">49.11</td>
</tr>
<tr>
<td align="left">Flashback</td>
<td align="left">47.33</td>
<td align="left">50.22</td>
</tr>
<tr>
<td align="left">GETNext</td>
<td align="left">47.15</td>
<td align="left">49.89</td>
</tr>
<tr>
<td align="left"><strong>MCLP</strong></td>
<td align="left"><strong>51.46</strong></td>
<td align="left"><strong>51.81</strong></td>
</tr>
</tbody></table>
<p><strong>关键结论</strong>：相比于将到达时间作为辅助任务的模型（如 GETNext），MCLP 这种显式将其作为上下文的方法表现更好，证明了时间规律对地点预测的直接决定作用。</p>
<h3><span id="消融实验">消融实验</span></h3><p><img src="https://private-us-east-1.manuscdn.com/sessionFile/Dmivc7PvROkeB5xQjeHeBc/sandbox/hvPd9uevNS53IFBWIfk4PO-images_1770790904325_na1fn_L2hvbWUvdWJ1bnR1L3BhcGVyLW5vdGVzL21jbHAvaW1hZ2VzL2ZpZzVfYWJsYXRpb25fdHJhZmZpYw.png?Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9wcml2YXRlLXVzLWVhc3QtMS5tYW51c2Nkbi5jb20vc2Vzc2lvbkZpbGUvRG1pdmM3UHZST2tlQjV4UWplSGVCYy9zYW5kYm94L2h2UGQ5dWV2TlM1M0lGQldJZms0UE8taW1hZ2VzXzE3NzA3OTA5MDQzMjVfbmExZm5fTDJodmJXVXZkV0oxYm5SMUwzQmhjR1Z5TFc1dmRHVnpMMjFqYkhBdmFXMWhaMlZ6TDJacFp6VmZZV0pzWVhScGIyNWZkSEpoWm1acFl3LnBuZyIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc5ODc2MTYwMH19fV19&Key-Pair-Id=K2HSFNDJXOU9YS&Signature=D8eVsypSTFYBKWma3hG364O1ltxQt5qorbkp3de6dMqmg7CtAnJ5snSSE3qMMznQsNzcMYsU6~v5BtZoVsf21z3afzdmuJAv~rhmBnSt0vcWQpy0a6lZtP6n5STymxTfVxfWugKDTzBG-A2v6DG36E-UDmDY5nzA3hzNfipEDJDByRl2Ps981JxsC1f1il49fjq4oH~yx9S6wbQMigBKBMZ8jNj82fnYLyyhOM0gAV7FrRJpjD~BNCsKaz9vmqzoqMg9Sy3Kvt1iPlr2~WxAxgYC0hejDj03YXWf5WuNEptUy~86nmYQ6x6TxzzB8f9t148fWQxRUxmu253B-DFwZA__" alt="消融实验结果"></p>
<p>实验证明，<code>+Pre</code>（用户偏好）和 <code>+At</code>（到达时间）组件均能独立提升模型性能，而两者结合时效果达到最优。</p>
<h2><span id="深度理解问答">深度理解问答</span></h2><h3><span id="q1-为什么使用-lda-提取偏好比直接让模型学习-embedding-更好">Q1: 为什么使用 LDA 提取偏好比直接让模型学习 Embedding 更好？</span></h3><p>直接学习的 Embedding（如传统模型中的 UserID Embedding）本质上是在学习一个 ID 的唯一表示，它需要大量的训练数据才能逐渐“悟”出用户的偏好。而 LDA 是基于全局统计规律的先验知识，它能直接识别出“这个用户经常去商场和电影院”这类高层语义，为模型提供了一个极佳的搜索起点，缓解了数据稀疏问题。</p>
<h3><span id="q2-到达时间估计器中为什么不直接预测一个具体的时间点">Q2: 到达时间估计器中，为什么不直接预测一个具体的时间点？</span></h3><p>人类的行为在时间上具有一定的“模糊性”。例如，你可能在 18:00 到 19:00 之间的任何时间去吃晚饭。如果模型只预测一个点，容错率极低。MCLP 通过注意力机制生成一个聚合的时间嵌入，本质上是捕获了用户在未来多个可能时段的概率分布，这种“软预测”作为上下文注入到地点预测中，比一个“硬预测”的数值包含更丰富的信息。</p>
<h3><span id="q3-模型在处理超长轨迹时表现如何">Q3: 模型在处理超长轨迹时表现如何？</span></h3><p>MCLP 采用了 Transformer 架构，理论上具有捕捉长距离依赖的能力。同时，通过 LDA 提取的全局偏好嵌入实际上起到了一种“长期记忆”的作用，弥补了 Transformer 在处理极长序列时的计算瓶颈或信息遗忘。</p>
<h2><span id="总结与思考">总结与思考</span></h2><h3><span id="核心贡献">核心贡献</span></h3><ul>
<li><strong>范式转变</strong>：将到达时间从“预测目标”转变为“预测依据”，利用了时间规律的强预测性。</li>
<li><strong>知识注入</strong>：通过主题模型引入先验偏好，提升了冷启动或稀疏数据下的表现。</li>
</ul>
<h3><span id="局限性">局限性</span></h3><ul>
<li><strong>计算开销</strong>：LDA 预处理和 Transformer 架构在超大规模实时系统中可能面临推理延迟挑战。</li>
<li><strong>动态偏好</strong>：LDA 提取的是静态全局偏好，对于用户短期内兴趣的剧烈转变（如出差、旅游）捕捉可能不够敏锐。</li>
</ul>
<h3><span id="适用场景">适用场景</span></h3><ul>
<li><strong>智慧城市交通规划</strong>：利用摄像头数据预测车辆去向。</li>
<li><strong>个性化 LBS 推荐</strong>：根据用户长期习惯和当前时间精准推送服务。</li>
</ul>
</div><div class="tags"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/" rel="tag">论文分享</a></li></ul></div><div class="post-nav"><a class="next" href="/posts/%E5%85%B3%E4%BA%8E%E8%BD%AC%E6%AD%A3%E5%A4%B1%E8%B4%A5%E6%88%91%E5%81%9A%E5%AF%B9%E4%BA%86%E5%93%AA%E4%BA%9B%E4%BA%8B%E6%83%85/">关于转正失败我做对了哪些事情</a></div></div></div></div><div class="pure-u-1 pure-u-md-4-4"><div id="footer">Copyright © 2026 <a href="/." rel="nofollow">RenaXu 's Wonderland.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="https://unpkg.com/@fancyapps/fancybox/dist/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="https://unpkg.com/@fancyapps/fancybox/dist/jquery.fancybox.min.css"><link rel="stylesheet" type="text/css" href="/css/search.css?v=1.0.0"><script type="text/javascript" src="/js/search.js?v=1.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
  search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="复制成功！"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script><script type="text/javascript" src="/js/toc.js?v=1.0.0"></script></div></body></html>